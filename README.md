# Databricks Local Development Pipeline

Delta Lake ê¸°ë°˜ì˜ ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ íŒŒì´í”„ë¼ì¸ ë¡œì»¬ ê°œë°œ í™˜ê²½

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” Databricksì˜ Medallion Architecture(Bronze-Silver-Gold)ë¥¼ ë¡œì»¬ í™˜ê²½ì—ì„œ êµ¬í˜„í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤. PySparkì™€ Delta Lakeë¥¼ í™œìš©í•˜ì—¬ í•­ê³µ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³ , PII(ê°œì¸ì‹ë³„ì •ë³´) ê²€ì¶œ ë° ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.

## ğŸ—ï¸ ì•„í‚¤í…ì²˜
```
Bronze Layer (Raw)
    â†“
Silver Layer (Cleaned & Validated)
    â†“
Gold Layer (Analytics & Mart)
    â†“
Governance & PII Detection
```

### ë°ì´í„° ë ˆì´ì–´

- **Bronze Layer**: ì›ë³¸ ë°ì´í„° ê·¸ëŒ€ë¡œ ì €ì¥ (Raw Data)
- **Silver Layer**: ë°ì´í„° ì •ì œ, ê²€ì¦, PII ë§ˆìŠ¤í‚¹
- **Gold Layer**: ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ìš© Mart í…Œì´ë¸”
- **Meta Layer**: PII ê²€ì¶œ ê²°ê³¼ ë° ê±°ë²„ë„ŒìŠ¤ ë©”íƒ€ë°ì´í„°

## ğŸš€ ì£¼ìš” ê¸°ëŠ¥

- âœ… Delta Lake ê¸°ë°˜ ACID íŠ¸ëœì­ì…˜
- âœ… íƒ€ì„ íŠ¸ë˜ë¸” (ë²„ì „ ê´€ë¦¬)
- âœ… ìŠ¤í‚¤ë§ˆ ì§„í™” (Schema Evolution)
- âœ… PII ìë™ ê²€ì¶œ ë° ë§ˆìŠ¤í‚¹
- âœ… ë°ì´í„° í’ˆì§ˆ ê²€ì¦
- âœ… Z-Ordering ìµœì í™”
- âœ… ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ ì •ì±…

## ğŸ“Š ë°ì´í„°ì…‹

### ìƒì„±ë˜ëŠ” í…Œì´ë¸”

| Layer | í…Œì´ë¸”ëª… | ì„¤ëª… | ë ˆì½”ë“œ ìˆ˜ |
|-------|---------|------|-----------|
| Bronze | `flights_raw` | í•­ê³µí¸ ì›ë³¸ ë°ì´í„° | 10,000 |
| Bronze | `customer_raw` | ê³ ê° ì›ë³¸ ë°ì´í„° (PII í¬í•¨) | 5,000 |
| Silver | `flights_silver` | ì •ì œëœ í•­ê³µí¸ ë°ì´í„° | 10,000 |
| Silver | `customer_silver` | PII ë§ˆìŠ¤í‚¹ëœ ê³ ê° ë°ì´í„° | 5,000 |
| Gold | `flight_delay_kpi` | í•­ê³µì‚¬ë³„ ì§€ì—° KPI | - |
| Gold | `route_performance` | ë…¸ì„ ë³„ ì„±ê³¼ ë¶„ì„ | - |
| Gold | `customer_segment_stats` | ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ í†µê³„ | - |
| Meta | `pii_detection_result` | PII ê²€ì¶œ ê²°ê³¼ | - |

## ğŸ› ï¸ ê¸°ìˆ  ìŠ¤íƒ

- **Python**: 3.8+
- **PySpark**: 3.5.0
- **Delta Lake**: 2.4.0
- **Apache Hadoop**: 3.3.6 (Windowsìš© winutils í¬í•¨)
- **Hive Metastore**: ë‚´ì¥ Derby DB

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°
```
databricks-local-pipeline/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ spark_config.py          # Spark ì„¸ì…˜ ë° ì„¤ì •
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ 01_generate_sample_data.py    # Bronze Layer ìƒì„±
â”‚   â”œâ”€â”€ 02_create_silver_layer.py     # Silver Layer ìƒì„±
â”‚   â”œâ”€â”€ 03_create_mart_layer.py       # Gold Layer ìƒì„±
â”‚   â”œâ”€â”€ 04_detect_pii.py              # PII ê²€ì¶œ
â”‚   â””â”€â”€ 05_apply_governance.py        # ê±°ë²„ë„ŒìŠ¤ ì •ì±…
â”œâ”€â”€ spark-warehouse/             # Delta Lake í…Œì´ë¸” ì €ì¥ì†Œ
â”œâ”€â”€ requirements.txt             # Python ì˜ì¡´ì„±
â”œâ”€â”€ .gitignore                   # Git ì œì™¸ íŒŒì¼
â””â”€â”€ README.md                    # í”„ë¡œì íŠ¸ ë¬¸ì„œ

ìƒì„±ë˜ëŠ” ë””ë ‰í† ë¦¬:
â”œâ”€â”€ metastore_db/               # Hive Metastore
â”œâ”€â”€ derby.log                   # Derby ë¡œê·¸
â””â”€â”€ venv/                       # Python ê°€ìƒí™˜ê²½
```

## ğŸ’» ì„¤ì¹˜ ë° ì‹¤í–‰

### 1. ì‚¬ì „ ìš”êµ¬ì‚¬í•­

#### Windows í™˜ê²½
- Python 3.8 ì´ìƒ
- Java 11 (PySpark ìš”êµ¬ì‚¬í•­)
- Hadoop winutils (Windows ì „ìš©)

#### Hadoop winutils ì„¤ì¹˜ (Windows)
```bash
# 1. winutils ë‹¤ìš´ë¡œë“œ
# https://github.com/cdarlint/winutils ì—ì„œ hadoop-3.3.6 ë²„ì „ ë‹¤ìš´ë¡œë“œ

# 2. ê²½ë¡œ ì„¤ì •
C:\hadoop-3.3.6\bin\winutils.exe

# 3. í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì„ íƒì‚¬í•­, ì½”ë“œì—ì„œ ìë™ ì„¤ì •ë¨)
HADOOP_HOME=C:\hadoop-3.3.6
```

### 2. í”„ë¡œì íŠ¸ ì„¤ì¹˜
```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/yourusername/databricks-local-pipeline.git
cd databricks-local-pipeline

# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv

# ê°€ìƒí™˜ê²½ í™œì„±í™”
# Windows
venv\Scripts\activate
# Linux/Mac
source venv/bin/activate

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r requirements.txt
```

### 3. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
```bash
# Step 1: Bronze Layer - ìƒ˜í”Œ ë°ì´í„° ìƒì„±
python scripts/01_generate_sample_data.py

# Step 2: Silver Layer - ë°ì´í„° ì •ì œ ë° ê²€ì¦
python scripts/02_create_silver_layer.py

# Step 3: Gold Layer - ë¶„ì„ìš© Mart ìƒì„±
python scripts/03_create_mart_layer.py

# Step 4: PII ê²€ì¶œ (ì„ íƒì‚¬í•­)
python scripts/04_detect_pii.py

# Step 5: ê±°ë²„ë„ŒìŠ¤ ì •ì±… ì ìš© (ì„ íƒì‚¬í•­)
python scripts/05_apply_governance.py
```

## ğŸ“– ì‚¬ìš© ì˜ˆì‹œ

### ë°ì´í„° ì¡°íšŒ
```python
from config.spark_config import *

# Spark ì„¸ì…˜ ìƒì„±
spark = get_spark_session("DataAnalysis")

# Bronze Layer ì¡°íšŒ
df_raw = spark.table("dev_air_raw.flights_raw")
df_raw.show(10)

# Silver Layer ì¡°íšŒ
df_silver = spark.table("dev_air_silver.flights_silver")
df_silver.show(10)

# SQL ì¿¼ë¦¬
spark.sql("""
    SELECT airline_code, 
           AVG(arr_delay) as avg_delay,
           COUNT(*) as flight_count
    FROM dev_air_silver.flights_silver
    GROUP BY airline_code
    ORDER BY avg_delay DESC
""").show()

# ì„¸ì…˜ ì¢…ë£Œ
stop_spark_session(spark)
```

### íƒ€ì„ íŠ¸ë˜ë¸”
```python
# íŠ¹ì • ë²„ì „ì˜ ë°ì´í„° ì¡°íšŒ
df_v0 = spark.read.format("delta") \
    .option("versionAsOf", 0) \
    .table("dev_air_silver.flights_silver")

# íŠ¹ì • ì‹œê°„ì˜ ë°ì´í„° ì¡°íšŒ
df_timestamp = spark.read.format("delta") \
    .option("timestampAsOf", "2025-12-31 13:00:00") \
    .table("dev_air_silver.flights_silver")

# íˆìŠ¤í† ë¦¬ ì¡°íšŒ
spark.sql("DESCRIBE HISTORY dev_air_silver.flights_silver").show()
```

## ğŸ”’ PII ë³´í˜¸

### ê²€ì¶œë˜ëŠ” PII ìœ í˜•

- âœ… ì´ë¦„ (Full Name)
- âœ… ì´ë©”ì¼ (Email)
- âœ… ì „í™”ë²ˆí˜¸ (Phone)
- âœ… ì—¬ê¶Œë²ˆí˜¸ (Passport Number)
- âœ… ì‹ ìš©ì¹´ë“œ ë²ˆí˜¸
- âœ… ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸/SSN

### ë§ˆìŠ¤í‚¹ ì˜ˆì‹œ
```python
# ì›ë³¸ (Bronze)
{
    "full_name": "John Doe",
    "email": "john.doe@example.com",
    "phone": "+1-555-123-4567"
}

# ë§ˆìŠ¤í‚¹ í›„ (Silver)
{
    "full_name": "J*** D***",
    "email": "j***@example.com",
    "phone": "+1-555-***-****"
}
```

## ğŸ§ª í…ŒìŠ¤íŠ¸
```bash
# ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
python -m pytest tests/

# ê°œë³„ ë ˆì´ì–´ í…ŒìŠ¤íŠ¸
python -m pytest tests/test_bronze_layer.py
python -m pytest tests/test_silver_layer.py
python -m pytest tests/test_pii_detection.py
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™”

- **Z-Ordering**: ìì£¼ í•„í„°ë§í•˜ëŠ” ì»¬ëŸ¼ ìµœì í™”
- **Partition Pruning**: ë‚ ì§œë³„ íŒŒí‹°ì…˜
- **Data Skipping**: Min/Max í†µê³„ í™œìš©
- **Caching**: ë°˜ë³µ ì¿¼ë¦¬ ì„±ëŠ¥ í–¥ìƒ

## ğŸ› ë¬¸ì œ í•´ê²°

### Windowsì—ì„œ "HADOOP_HOME is not set" ì˜¤ë¥˜
```python
# spark_config.pyì—ì„œ ìë™ ì²˜ë¦¬ë¨
# ë˜ëŠ” ìˆ˜ë™ìœ¼ë¡œ í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
import os
os.environ['HADOOP_HOME'] = r'C:\hadoop-3.3.6'
```

### "Python worker failed to connect back" ì˜¤ë¥˜
```python
# spark_config.pyì— Python ê²½ë¡œ ëª…ì‹œë¨
.config("spark.pyspark.python", sys.executable)
.config("spark.pyspark.driver.python", sys.executable)
```

### ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ ê²½ê³ 
```
# ë¬´ì‹œí•´ë„ ë¨ - Windows íŒŒì¼ ì ê¸ˆ ë¬¸ì œ
# ì‹¤ì œ ë°ì´í„°ë‚˜ ì‹¤í–‰ì—ëŠ” ì˜í–¥ ì—†ìŒ
```

## ğŸ“ ë¼ì´ì„ ìŠ¤

MIT License

## ğŸ‘¥ ê¸°ì—¬

Pull RequestëŠ” ì–¸ì œë‚˜ í™˜ì˜í•©ë‹ˆë‹¤!

1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ“§ ì—°ë½ì²˜

í”„ë¡œì íŠ¸ ê´€ë¦¬ì: ì±„ì§„ê¸°(fbg6455@naver.com)

í”„ë¡œì íŠ¸ ë§í¬: https://github.com/Jinkichae/Databricks-Local-Development-Pipeline

## ğŸ™ ê°ì‚¬ì˜ ê¸€

- [Apache Spark](https://spark.apache.org/)
- [Delta Lake](https://delta.io/)
- [Databricks](https://www.databricks.com/)

## ğŸ“š ì¶”ê°€ ìë£Œ

- [Delta Lake Documentation](https://docs.delta.io/)
- [PySpark Documentation](https://spark.apache.org/docs/latest/api/python/)
- [Data Governance Best Practices](https://www.databricks.com/glossary/data-governance)
- [GDPR Compliance Guide](https://gdpr.eu/)
