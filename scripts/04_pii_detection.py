"""
04. PII ê²€ì¶œ ìŠ¤í¬ë¦½íŠ¸
Rule ê¸°ë°˜ + AI ê¸°ë°˜ PII ê²€ì¶œ íŒŒì´í”„ë¼ì¸
"""

import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.spark_config import *
from pyspark.sql import functions as F
from pyspark.sql.types import StringType
import re

# PII ê²€ì¶œ ê·œì¹™ ì •ì˜
EMAIL_PATTERN = r'^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}$'
PHONE_PATTERN = r'^\+?\d{1,3}[-\s]?\(?\d{3}\)?[-\s]?\d{3}[-\s]?\d{4}$'
PASSPORT_PATTERN = r'^P\d{8}$'


def rule_based_pii_detection(spark):
    """Rule ê¸°ë°˜ PII ê²€ì¶œ"""
    print("Running Rule-based PII Detection...")

    df_customer = spark.table(TABLE_CUSTOMER_SILVER)

    # ê° ì»¬ëŸ¼ë³„ PII íƒ€ì… íŒë³„
    df_pii = df_customer.select(
        'customer_id',
        'full_name',
        'email',
        'phone',
        'passport_no',

        # Email ê²€ì¦
        F.when(F.col('email').rlike(EMAIL_PATTERN), 'EMAIL')
        .otherwise('INVALID').alias('email_pii_type'),

        # Phone ê²€ì¦
        F.when(F.col('phone').rlike(PHONE_PATTERN), 'PHONE')
        .otherwise('INVALID').alias('phone_pii_type'),

        # Passport ê²€ì¦
        F.when(F.col('passport_no').rlike(PASSPORT_PATTERN), 'PASSPORT')
        .otherwise('INVALID').alias('passport_pii_type'),

        # Full Nameì€ í•­ìƒ PII
        F.lit('NAME').alias('name_pii_type')
    )

    # PII ê²€ì¶œ í†µê³„
    print("\nğŸ“Š Rule-based Detection Results:")
    print(f"   Valid Emails:    {df_pii.filter(F.col('email_pii_type') == 'EMAIL').count():,}")
    print(f"   Valid Phones:    {df_pii.filter(F.col('phone_pii_type') == 'PHONE').count():,}")
    print(f"   Valid Passports: {df_pii.filter(F.col('passport_pii_type') == 'PASSPORT').count():,}")

    return df_pii


def ai_based_pii_classifier(text):
    """
    AI ê¸°ë°˜ PII ë¶„ë¥˜ê¸° (ì‹œë®¬ë ˆì´ì…˜)
    ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” NER ëª¨ë¸ì´ë‚˜ LLMì„ ì‚¬ìš©
    """
    if text is None or text == '':
        return 'NONE'

    text_lower = text.lower()

    # Email íŒ¨í„´
    if '@' in text and '.' in text:
        if re.match(EMAIL_PATTERN, text):
            return 'EMAIL'
        else:
            return 'EMAIL_LIKE'

    # Phone íŒ¨í„´
    if re.search(r'\d{3}[-\s]?\d{3}[-\s]?\d{4}', text):
        return 'PHONE'

    # Passport íŒ¨í„´
    if text.startswith('P') and len(text) == 9:
        return 'PASSPORT'

    # ì´ë¦„ íŒ¨í„´ (ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ 2ê°œ ì´ìƒì˜ ë‹¨ì–´)
    words = text.split()
    if len(words) >= 2 and all(w.isalpha() for w in words):
        return 'NAME'

    # ID íŒ¨í„´
    if text.startswith('CUST') and len(text) == 10:
        return 'CUSTOMER_ID'

    return 'UNKNOWN'


def ai_based_pii_detection(spark, df_rule_based):
    """AI ê¸°ë°˜ PII ê²€ì¶œ (UDF ì‚¬ìš©)"""
    print("\nRunning AI-based PII Detection...")

    # UDF ë“±ë¡
    pii_classifier_udf = F.udf(ai_based_pii_classifier, StringType())

    # AI ë¶„ë¥˜ ì ìš©
    df_ai = df_rule_based.select(
        'customer_id',
        'full_name',
        'email',
        'phone',
        'passport_no',
        'email_pii_type',
        'phone_pii_type',
        'passport_pii_type',
        'name_pii_type',

        # AI ê¸°ë°˜ ë¶„ë¥˜ ì¶”ê°€
        pii_classifier_udf('full_name').alias('ai_name_type'),
        pii_classifier_udf('email').alias('ai_email_type'),
        pii_classifier_udf('phone').alias('ai_phone_type'),
        pii_classifier_udf('passport_no').alias('ai_passport_type'),

        # ê²€ì¶œ ì‹ ë¢°ë„ ê³„ì‚°
        F.when(
            (F.col('email_pii_type') == pii_classifier_udf('email')) &
            (F.col('phone_pii_type') == pii_classifier_udf('phone')) &
            (F.col('passport_pii_type') == pii_classifier_udf('passport_no')),
            'HIGH'
        ).when(
            (F.col('email_pii_type') == pii_classifier_udf('email')) |
            (F.col('phone_pii_type') == pii_classifier_udf('phone')),
            'MEDIUM'
        ).otherwise('LOW').alias('detection_confidence'),

        F.current_timestamp().alias('detection_timestamp')
    )

    # í†µê³„
    print("\nğŸ“Š AI-based Detection Results:")
    confidence_dist = df_ai.groupBy('detection_confidence').count().collect()
    for row in confidence_dist:
        print(f"   {row['detection_confidence']} confidence: {row['count']:,}")

    return df_ai


def save_pii_detection_results(spark, df_pii):
    """PII ê²€ì¶œ ê²°ê³¼ ì €ì¥"""
    print("\nSaving PII detection results...")

    df_pii.write \
        .format("delta") \
        .mode("overwrite") \
        .saveAsTable(TABLE_PII_DETECTION)

    count = df_pii.count()
    print(f"âœ“ Saved {count:,} records â†’ {TABLE_PII_DETECTION}")

    return count


def create_masked_view(spark):
    """ë§ˆìŠ¤í‚¹ëœ ê³ ê° ë·° ìƒì„±"""
    print("\nCreating masked customer view...")

    spark.sql(f"""
        CREATE OR REPLACE VIEW {DB_SILVER}.customer_masked_view AS
        SELECT
            customer_id,
            CONCAT(SUBSTRING(full_name, 1, 1), '***') AS full_name_masked,
            CONCAT(SUBSTRING(email, 1, 3), '***@***', 
                   SUBSTRING_INDEX(email, '@', -1)) AS email_masked,
            CONCAT(SUBSTRING(phone, 1, 6), '***-****') AS phone_masked,
            '***MASKED***' AS passport_masked,
            country,
            segment,
            transformation_timestamp
        FROM {TABLE_CUSTOMER_SILVER}
    """)

    print(f"âœ“ Created view: {DB_SILVER}.customer_masked_view")

    # ë§ˆìŠ¤í‚¹ ì˜ˆì‹œ
    print("\nğŸ”’ Masked Data Sample:")
    spark.sql(f"SELECT * FROM {DB_SILVER}.customer_masked_view LIMIT 3").show(truncate=False)


def generate_pii_report(spark):
    """PII ê²€ì¶œ ë¦¬í¬íŠ¸ ìƒì„±"""
    print("\n" + "=" * 60)
    print("PII DETECTION REPORT")
    print("=" * 60)

    # ì „ì²´ í†µê³„
    print("\nğŸ“Š Overall Statistics:")
    overall = spark.sql(f"""
        SELECT 
            COUNT(*) AS total_records,
            SUM(CASE WHEN detection_confidence = 'HIGH' THEN 1 ELSE 0 END) AS high_confidence,
            SUM(CASE WHEN detection_confidence = 'MEDIUM' THEN 1 ELSE 0 END) AS medium_confidence,
            SUM(CASE WHEN detection_confidence = 'LOW' THEN 1 ELSE 0 END) AS low_confidence
        FROM {TABLE_PII_DETECTION}
    """).collect()[0]

    print(f"   Total Records:      {overall['total_records']:,}")
    print(
        f"   High Confidence:    {overall['high_confidence']:,} ({overall['high_confidence'] * 100 / overall['total_records']:.1f}%)")
    print(
        f"   Medium Confidence:  {overall['medium_confidence']:,} ({overall['medium_confidence'] * 100 / overall['total_records']:.1f}%)")
    print(
        f"   Low Confidence:     {overall['low_confidence']:,} ({overall['low_confidence'] * 100 / overall['total_records']:.1f}%)")

    # PII íƒ€ì…ë³„ ë¶„í¬
    print("\nğŸ“‹ PII Type Distribution:")
    spark.sql(f"""
        SELECT 
            'Email' AS pii_type,
            COUNT(*) AS count
        FROM {TABLE_PII_DETECTION}
        WHERE email_pii_type = 'EMAIL'

        UNION ALL

        SELECT 
            'Phone' AS pii_type,
            COUNT(*) AS count
        FROM {TABLE_PII_DETECTION}
        WHERE phone_pii_type = 'PHONE'

        UNION ALL

        SELECT 
            'Passport' AS pii_type,
            COUNT(*) AS count
        FROM {TABLE_PII_DETECTION}
        WHERE passport_pii_type = 'PASSPORT'

        UNION ALL

        SELECT 
            'Name' AS pii_type,
            COUNT(*) AS count
        FROM {TABLE_PII_DETECTION}
        WHERE name_pii_type = 'NAME'
    """).show(truncate=False)

    # ê²€ì¶œ ë¶ˆì¼ì¹˜ ì¼€ì´ìŠ¤
    print("\nâš ï¸  Rule vs AI Discrepancies:")
    discrepancies = spark.sql(f"""
        SELECT COUNT(*) AS count
        FROM {TABLE_PII_DETECTION}
        WHERE email_pii_type != ai_email_type
           OR phone_pii_type != ai_phone_type
           OR passport_pii_type != ai_passport_type
    """).collect()[0]['count']

    print(f"   Discrepant Records: {discrepancies:,}")

    if discrepancies > 0:
        print("\n   Sample Discrepancies:")
        spark.sql(f"""
            SELECT 
                customer_id,
                email_pii_type,
                ai_email_type,
                phone_pii_type,
                ai_phone_type
            FROM {TABLE_PII_DETECTION}
            WHERE email_pii_type != ai_email_type
               OR phone_pii_type != ai_phone_type
            LIMIT 5
        """).show(truncate=False)


def main():
    print("=" * 60)
    print("STEP 4: PII Detection Pipeline")
    print("=" * 60)
    print()

    # Spark ì„¸ì…˜ ìƒì„±
    spark = get_spark_session("04_PIIDetection")

    # Rule ê¸°ë°˜ ê²€ì¶œ
    df_rule = rule_based_pii_detection(spark)

    # AI ê¸°ë°˜ ê²€ì¶œ
    df_ai = ai_based_pii_detection(spark, df_rule)

    # ê²°ê³¼ ì €ì¥
    result_count = save_pii_detection_results(spark, df_ai)

    # ë§ˆìŠ¤í‚¹ ë·° ìƒì„±
    create_masked_view(spark)

    # ë¦¬í¬íŠ¸ ìƒì„±
    generate_pii_report(spark)

    # ìš”ì•½
    print("\n" + "=" * 60)
    print("âœ… PII DETECTION COMPLETE")
    print("=" * 60)
    print(f"\nğŸ” Detection Summary:")
    print(f"   â€¢ Records scanned: {result_count:,}")
    print(f"   â€¢ Methods used: Rule-based + AI-based")
    print(f"   â€¢ Confidence levels: HIGH, MEDIUM, LOW")
    print(f"\nğŸ“ Outputs:")
    print(f"   â€¢ {TABLE_PII_DETECTION}")
    print(f"   â€¢ {DB_SILVER}.customer_masked_view")
    print(f"\nğŸ”’ Data Governance:")
    print(f"   â€¢ PII identified and catalogued")
    print(f"   â€¢ Masked views created")
    print(f"   â€¢ Ready for access control")
    print(f"\nğŸš€ Next step: python scripts/05_analytics_queries.py")
    print()

    # Spark ì„¸ì…˜ ì¢…ë£Œ
    stop_spark_session(spark)


if __name__ == "__main__":
    main()