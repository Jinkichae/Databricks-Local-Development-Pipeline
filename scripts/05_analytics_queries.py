"""
05. ë¶„ì„ ì¿¼ë¦¬ ìŠ¤í¬ë¦½íŠ¸
Mart ë ˆì´ì–´ë¥¼ í™œìš©í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ ì¿¼ë¦¬ ì‹¤í–‰
"""

import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.spark_config import *


def airline_performance_analysis(spark):
    """í•­ê³µì‚¬ ì„±ê³¼ ë¶„ì„"""
    print("=" * 60)
    print("1ï¸âƒ£  AIRLINE PERFORMANCE ANALYSIS")
    print("=" * 60)

    # í•­ê³µì‚¬ë³„ ì „ì²´ ì„±ê³¼
    print("\nğŸ“Š Overall Airline Performance:")
    spark.sql(f"""
        SELECT
            airline_code,
            total_flights,
            CONCAT(ROUND(avg_arr_delay, 1), ' min') AS avg_delay,
            CONCAT(delayed_rate_pct, '%') AS delay_rate,
            CONCAT(ROUND(avg_distance_miles, 0), ' mi') AS avg_distance,
            CASE 
                WHEN delayed_rate_pct < 20 THEN 'â­â­â­â­â­'
                WHEN delayed_rate_pct < 25 THEN 'â­â­â­â­'
                WHEN delayed_rate_pct < 30 THEN 'â­â­â­'
                ELSE 'â­â­'
            END AS rating
        FROM {TABLE_FLIGHT_DELAY_KPI}
        ORDER BY delayed_rate_pct ASC
    """).show(truncate=False)

    # ì§€ì—° ì‹œê°„ ë¶„í¬
    print("\nğŸ“ˆ Delay Time Distribution:")
    spark.sql(f"""
        SELECT
            airline_code,
            median_arr_delay AS median_delay,
            max_arr_delay AS max_delay,
            min_arr_delay AS min_delay,
            max_arr_delay - min_arr_delay AS delay_range
        FROM {TABLE_FLIGHT_DELAY_KPI}
        ORDER BY median_delay DESC
        LIMIT 5
    """).show(truncate=False)


def route_analysis(spark):
    """ë…¸ì„  ë¶„ì„"""
    print("\n" + "=" * 60)
    print("2ï¸âƒ£  ROUTE ANALYSIS")
    print("=" * 60)

    # ê°€ì¥ ë°”ìœ ë…¸ì„ 
    print("\nğŸ›« Top 10 Busiest Routes:")
    spark.sql(f"""
        SELECT
            route,
            total_flights,
            CONCAT(ROUND(distance_miles, 0), ' mi') AS distance,
            CONCAT(ROUND(avg_arr_delay, 1), ' min') AS avg_delay,
            airlines_serving AS airlines,
            CASE 
                WHEN delayed_rate_pct < 20 THEN 'âœ… Good'
                WHEN delayed_rate_pct < 30 THEN 'âš ï¸  Fair'
                ELSE 'âŒ Poor'
            END AS performance
        FROM {TABLE_ROUTE_PERFORMANCE}
        ORDER BY total_flights DESC
        LIMIT 10
    """).show(truncate=False)

    # ê°€ì¥ ì§€ì—°ì´ ì‹¬í•œ ë…¸ì„ 
    print("\nâ° Top 10 Most Delayed Routes:")
    spark.sql(f"""
        SELECT
            route,
            total_flights,
            CONCAT(ROUND(avg_arr_delay, 1), ' min') AS avg_delay,
            CONCAT(delayed_rate_pct, '%') AS delay_rate,
            delayed_flights
        FROM {TABLE_ROUTE_PERFORMANCE}
        WHERE total_flights >= 50
        ORDER BY avg_arr_delay DESC
        LIMIT 10
    """).show(truncate=False)

    # ê°€ì¥ íš¨ìœ¨ì ì¸ ë…¸ì„ 
    print("\nâš¡ Top 10 Most Efficient Routes:")
    spark.sql(f"""
        SELECT
            route,
            total_flights,
            CONCAT(ROUND(avg_arr_delay, 1), ' min') AS avg_delay,
            CONCAT(delayed_rate_pct, '%') AS delay_rate
        FROM {TABLE_ROUTE_PERFORMANCE}
        WHERE total_flights >= 50
        ORDER BY avg_arr_delay ASC
        LIMIT 10
    """).show(truncate=False)


def customer_segment_analysis(spark):
    """ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ ë¶„ì„"""
    print("\n" + "=" * 60)
    print("3ï¸âƒ£  CUSTOMER SEGMENT ANALYSIS")
    print("=" * 60)

    # ì„¸ê·¸ë¨¼íŠ¸ë³„ ì „ì²´ ë¶„í¬
    print("\nğŸ‘¥ Customer Distribution by Segment:")
    spark.sql(f"""
        SELECT
            segment,
            SUM(customer_count) AS total_customers,
            COUNT(DISTINCT country) AS countries,
            ROUND(SUM(customer_count) * 100.0 / 
                  (SELECT SUM(customer_count) FROM {TABLE_CUSTOMER_SEGMENT_STATS}), 2) AS percentage
        FROM {TABLE_CUSTOMER_SEGMENT_STATS}
        GROUP BY segment
        ORDER BY total_customers DESC
    """).show(truncate=False)

    # êµ­ê°€ë³„ TOP ì„¸ê·¸ë¨¼íŠ¸
    print("\nğŸŒ Top Segment by Country (Top 10 Countries):")
    spark.sql(f"""
        WITH ranked AS (
            SELECT
                country,
                segment,
                customer_count,
                ROW_NUMBER() OVER (PARTITION BY country ORDER BY customer_count DESC) AS rn
            FROM {TABLE_CUSTOMER_SEGMENT_STATS}
        )
        SELECT
            country,
            segment AS top_segment,
            customer_count,
            pct_in_country AS pct
        FROM ranked JOIN {TABLE_CUSTOMER_SEGMENT_STATS} USING (country, segment, customer_count)
        WHERE rn = 1
        ORDER BY customer_count DESC
        LIMIT 10
    """).show(truncate=False)

    # í”„ë¦¬ë¯¸ì—„ ê³ ê° ë¶„ì„
    print("\nğŸ’ Premium Customer Analysis (Business + First Class):")
    spark.sql(f"""
        SELECT
            country,
            SUM(CASE WHEN segment IN ('Business', 'First Class') THEN customer_count ELSE 0 END) AS premium_customers,
            SUM(customer_count) AS total_customers,
            ROUND(SUM(CASE WHEN segment IN ('Business', 'First Class') THEN customer_count ELSE 0 END) * 100.0 
                  / SUM(customer_count), 2) AS premium_pct
        FROM {TABLE_CUSTOMER_SEGMENT_STATS}
        GROUP BY country
        HAVING premium_customers > 0
        ORDER BY premium_pct DESC
        LIMIT 10
    """).show(truncate=False)


def cross_analysis(spark):
    """êµì°¨ ë¶„ì„"""
    print("\n" + "=" * 60)
    print("4ï¸âƒ£  CROSS ANALYSIS")
    print("=" * 60)

    # í•­ê³µì‚¬-ë…¸ì„  êµì°¨ ë¶„ì„
    print("\nğŸ” Airline Performance on Busiest Routes:")
    spark.sql(f"""
        SELECT
            r.route,
            COUNT(DISTINCT f.airline_code) AS airlines,
            r.total_flights,
            ROUND(AVG(k.avg_arr_delay), 1) AS avg_airline_delay,
            ROUND(r.avg_arr_delay, 1) AS route_avg_delay
        FROM {TABLE_ROUTE_PERFORMANCE} r
        JOIN {TABLE_FLIGHTS_SILVER} f 
            ON r.origin_airport = f.origin_airport 
            AND r.dest_airport = f.dest_airport
        JOIN {TABLE_FLIGHT_DELAY_KPI} k 
            ON f.airline_code = k.airline_code
        WHERE r.total_flights >= 100
        GROUP BY r.route, r.total_flights, r.avg_arr_delay
        ORDER BY r.total_flights DESC
        LIMIT 10
    """).show(truncate=False)


def operational_metrics(spark):
    """ìš´ì˜ ë©”íŠ¸ë¦­"""
    print("\n" + "=" * 60)
    print("5ï¸âƒ£  OPERATIONAL METRICS")
    print("=" * 60)

    # ì „ì²´ ìš´ì˜ í†µê³„
    print("\nğŸ“Š Overall Operational Statistics:")
    result = spark.sql(f"""
        SELECT
            SUM(total_flights) AS total_flights,
            SUM(delayed_flights) AS total_delayed,
            ROUND(SUM(delayed_flights) * 100.0 / SUM(total_flights), 2) AS overall_delay_rate,
            ROUND(AVG(avg_arr_delay), 2) AS avg_delay_minutes,
            SUM(total_distance_miles) AS total_miles,
            COUNT(DISTINCT airline_code) AS airlines
        FROM {TABLE_FLIGHT_DELAY_KPI}
    """).collect()[0]

    print(f"""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘  Total Flights:          {result['total_flights']:>10,}              â•‘
    â•‘  Delayed Flights:        {result['total_delayed']:>10,}              â•‘
    â•‘  Overall Delay Rate:     {result['overall_delay_rate']:>10.2f}%            â•‘
    â•‘  Avg Delay Time:         {result['avg_delay_minutes']:>10.2f} minutes      â•‘
    â•‘  Total Miles Flown:      {result['total_miles']:>10,.0f}          â•‘
    â•‘  Airlines Operating:     {result['airlines']:>10}              â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # íš¨ìœ¨ì„± ë²¤ì¹˜ë§ˆí¬
    print("\nğŸ¯ Efficiency Benchmark:")
    spark.sql(f"""
        SELECT
            'Best Performer' AS category,
            airline_code,
            delayed_rate_pct AS metric
        FROM {TABLE_FLIGHT_DELAY_KPI}
        ORDER BY delayed_rate_pct ASC
        LIMIT 1

        UNION ALL

        SELECT
            'Worst Performer' AS category,
            airline_code,
            delayed_rate_pct AS metric
        FROM {TABLE_FLIGHT_DELAY_KPI}
        ORDER BY delayed_rate_pct DESC
        LIMIT 1

        UNION ALL

        SELECT
            'Industry Average' AS category,
            'N/A' AS airline_code,
            ROUND(AVG(delayed_rate_pct), 2) AS metric
        FROM {TABLE_FLIGHT_DELAY_KPI}
    """).show(truncate=False)


def export_summary_report(spark):
    """ìš”ì•½ ë¦¬í¬íŠ¸ ë‚´ë³´ë‚´ê¸°"""
    print("\n" + "=" * 60)
    print("6ï¸âƒ£  SUMMARY REPORT")
    print("=" * 60)

    # ë°ì´í„° ë ˆì´ì–´ ìš”ì•½
    print("\nğŸ“ Data Layer Summary:")
    layers = [
        ('Bronze/Raw', DB_RAW, [TABLE_FLIGHTS_RAW, TABLE_CUSTOMER_RAW]),
        ('Silver/Curated', DB_SILVER, [TABLE_FLIGHTS_SILVER, TABLE_CUSTOMER_SILVER]),
        ('Gold/Mart', DB_MART, [TABLE_FLIGHT_DELAY_KPI, TABLE_ROUTE_PERFORMANCE, TABLE_CUSTOMER_SEGMENT_STATS]),
        ('Metadata', DB_META, [TABLE_PII_DETECTION])
    ]

    for layer_name, db, tables in layers:
        print(f"\n  {layer_name}:")
        for table in tables:
            try:
                count = spark.table(table).count()
                table_name = table.split('.')[-1]
                print(f"    â€¢ {table_name:30} {count:>10,} rows")
            except:
                print(f"    â€¢ {table_name:30} {'N/A':>10}")

    # ì£¼ìš” ì¸ì‚¬ì´íŠ¸
    print("\nğŸ’¡ Key Insights:")

    # ìµœê³  í•­ê³µì‚¬
    best_airline = spark.sql(f"""
        SELECT airline_code, delayed_rate_pct
        FROM {TABLE_FLIGHT_DELAY_KPI}
        ORDER BY delayed_rate_pct ASC
        LIMIT 1
    """).collect()[0]

    # ê°€ì¥ ë°”ìœ ë…¸ì„ 
    busiest_route = spark.sql(f"""
        SELECT route, total_flights
        FROM {TABLE_ROUTE_PERFORMANCE}
        ORDER BY total_flights DESC
        LIMIT 1
    """).collect()[0]

    # ìµœëŒ€ ê³ ê° ì„¸ê·¸ë¨¼íŠ¸
    top_segment = spark.sql(f"""
        SELECT segment, SUM(customer_count) AS total
        FROM {TABLE_CUSTOMER_SEGMENT_STATS}
        GROUP BY segment
        ORDER BY total DESC
        LIMIT 1
    """).collect()[0]

    print(f"""
    1. Best Performing Airline: {best_airline['airline_code']} ({best_airline['delayed_rate_pct']}% delay rate)
    2. Busiest Route: {busiest_route['route']} ({busiest_route['total_flights']:,} flights)
    3. Largest Customer Segment: {top_segment['segment']} ({top_segment['total']:,} customers)
    """)


def main():
    print("=" * 60)
    print("STEP 5: Analytics & Business Intelligence")
    print("=" * 60)
    print()

    # Spark ì„¸ì…˜ ìƒì„±
    spark = get_spark_session("05_AnalyticsQueries")

    # ê° ë¶„ì„ ì‹¤í–‰
    airline_performance_analysis(spark)
    route_analysis(spark)
    customer_segment_analysis(spark)
    cross_analysis(spark)
    operational_metrics(spark)
    export_summary_report(spark)

    # ìµœì¢… ìš”ì•½
    print("\n" + "=" * 60)
    print("âœ… ANALYTICS PIPELINE COMPLETE")
    print("=" * 60)
    print(f"""
    ğŸ‰ All Analysis Complete!

    ğŸ“Š Analyses Performed:
       â€¢ Airline Performance Analysis
       â€¢ Route Performance Analysis  
       â€¢ Customer Segment Analysis
       â€¢ Cross-dimensional Analysis
       â€¢ Operational Metrics

    ğŸ¯ Business Value:
       â€¢ Executive dashboards
       â€¢ Performance monitoring
       â€¢ Strategic decision support
       â€¢ Customer insights

    ğŸ“ˆ Data Architecture:
       Bronze (Raw) â†’ Silver (Curated) â†’ Gold (Mart)

    ğŸ”’ Data Governance:
       â€¢ PII detection completed
       â€¢ Masked views available
       â€¢ Access controls ready

    ğŸ“‚ Project Location: ./airlines-pyspark-demo/
    ğŸ“ Data Location: ./spark-warehouse/

    âœ¨ Portfolio Highlights:
       âœ“ PySpark + Delta Lake
       âœ“ Multi-layer data architecture
       âœ“ Data quality & governance
       âœ“ Business analytics
       âœ“ Production-ready code
    """)

    # Spark ì„¸ì…˜ ì¢…ë£Œ
    stop_spark_session(spark)


if __name__ == "__main__":
    main()